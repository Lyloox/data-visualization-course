{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "Random Forests are a bunch of decision trees combined.\n",
    "\n",
    "It's called a **ensemble**. \n",
    "\n",
    "The idea of *ensemble classifiers* is to combine several weak classifiers into one strong classifier which results in a better performance. \n",
    "\n",
    "There is different methods to combine classifiers. Random forests are built using a *ensemble method* called **bagging**, in which multiple decision trees are used in parallel. The random forest then takes the mean value of the results from each decision tree.\n",
    "\n",
    "Random forests reduce the risk of overfitting and accuracy is usually much higher than a single decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\"\"\"\n",
    "`import sklearn` does not automatically import every submodules, \n",
    "you may have to import them explicitely like above.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jyquickhelper import add_notebook_menu\n",
    "print(\"Table of Contents\")\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_data = pd.read_csv(\"Melbourne_housing_FULL.csv\")\n",
    "melbourne_data = melbourne_data.dropna(axis=0).reset_index()\n",
    "\n",
    "features = ['Rooms', 'Bathroom', 'Landsize', \"Car\", \"YearBuilt\"]\n",
    "\n",
    "y = melbourne_data.Price\n",
    "X = melbourne_data[features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestRegressor(random_state=42)\n",
    "forest_model.fit(X_train, y_train)\n",
    "predictions = forest_model.predict(X_test)\n",
    "mae = sklearn.metrics.mean_absolute_error(y_test, predictions)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of BaggingClassifier with DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n",
    "                        random_state=1)\n",
    "bag = bag.fit(X, y)\n",
    "predictions = bag.predict(X_test)\n",
    "mae = sklearn.metrics.mean_absolute_error(y_test, predictions)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! It took more time to create the model, but the MAE is now largely reduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the RandomForest, you will need to [install GraphViz](https://graphviz.gitlab.io/download/).\n",
    "\n",
    "Simple instructions:\n",
    "* On Linux (Ubuntu): `apt install graphviz`\n",
    "* On MacOS: `brew install graphviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "# Train\n",
    "model.fit(X, y)\n",
    "# Extract single tree\n",
    "estimator = model.estimators_[5]\n",
    "\n",
    "# Export as dot file\n",
    "sklearn.tree.export_graphviz(estimator, out_file='tree.dot', \n",
    "                max_depth=5,\n",
    "                feature_names = features,\n",
    "                rounded = True, proportion = True, \n",
    "                precision = 2, filled = True, rotate=True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More models\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "Linear Regression is one of the most simple model.\n",
    "\n",
    "Its goal (objective function) is to minimize the error, with targets predicted by linear approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "mae = sklearn.metrics.mean_absolute_error(y_test, predictions)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "A model built on Linear Regression, but with a different objective function, using L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "mae = sklearn.metrics.mean_absolute_error(y_test, predictions)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoosting Regression\n",
    "\n",
    "Another ensemble method, alongside bagging, is **boosting**. \n",
    "\n",
    "Instead of running parallel classifiers, it builds a classifier on the data set, and then a second classifier on the part of the data that the first classifier did not correctly predict.\n",
    "\n",
    "It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "mae = sklearn.metrics.mean_absolute_error(y_test, predictions)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was a short lesson, and without any TODO's.\n",
    "\n",
    "It's because I want you to start `project02` as soon as possible, and do the most of what you can!\n",
    "\n",
    "See you there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
