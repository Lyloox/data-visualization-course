{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Dataset Project\n",
    "\n",
    "The Iris Dataset is a basic dataset usually used to show data analysis techniques and machine learning algorithms.\n",
    "\n",
    "This Notebook will be both your lesson & project for next session.\n",
    "\n",
    "## Constraints\n",
    "* Fill all the # TODO cells.\n",
    "* When asked to do visualization, use plotting from **matplotlib**, not pandas or seaborn. Expect explicity authorized.\n",
    "* Send your work to my inbox laure.daumal@ext.devinci.fr\n",
    "* **Deadline**: Wednesday, 11th of March, 23h42. You have a bit more than one week and a half.\n",
    "\n",
    "Don't forget to\n",
    "1. Clear all outputs before saving & sending it to me (**Cell >> All Output >> Clear**).\n",
    "* Restart the kernel and run all cells to check no errors are found (**Cell >> Run All**).\n",
    "\n",
    "# Features Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns  #  <-- A new library!\n",
    "from sklearn.preprocessing import StandardScaler  # <-- A new method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "\n",
    "df = pd.read_csv(iris_dataset, names=['SepalLength (cm)', \n",
    "                                      'SepalWidth (cm)',\n",
    "                                      'PetalLength (cm)',\n",
    "                                      'PetalWidth (cm)',\n",
    "                                      'Species'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 150 samples, defined by 4 features. \n",
    "\n",
    "The `Species` does not count as a feature because it is usually used as a **target** for classification.\n",
    "\n",
    "Classification consist of creating a model that takes a sample of the 4 features given in centimeter and return the corresponding target, here an Iris species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Species\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will see some cool usage of the **`seaborn`** package.\n",
    "\n",
    "For example, we can usually plot a bar count from a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('Species', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sepal Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swarm plot\n",
    "\n",
    "`swarmplot` method from `seaborn`: a kind of plotting we didn't see before: it's like a swarm of bees buzzing abount their hive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(x=\"Species\", y=\"SepalLength (cm)\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(x=\"Species\", y=\"SepalWidth (cm)\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to visualize each Iris feature in 2D.\n",
    "\n",
    "Your task: Display a 2D visualization of each sample based on their SepalLength and SepalWidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print a matplotlib 2D visualization of each sample based on their SepalLength and SepalWidth. \n",
    "# Hint: Use a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use your previous visualization and change the color according to each sample's Species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent can also be obtained with `seaborn` using `scatterplot` and the `hue` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"SepalLength (cm)\", y=\"SepalWidth (cm)\", hue=\"Species\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use the method `lmplot` from `seaborn`: It uses a linear regression model across a `FacetGrid` to represent the relationship between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"SepalLength (cm)\", y=\"SepalWidth (cm)\", hue=\"Species\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Petal Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the previous matplotlib scatter visualization, but now show the relationship between the PepalLength and PepalWidth \n",
    "# (with the same species's colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the equivalent from Seaborn with Linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"PetalLength (cm)\", y=\"PetalWidth (cm)\", hue=\"Species\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Features Exploration\n",
    "\n",
    "\n",
    "### Histograms\n",
    "For each of the 4 features, show its distribution in a Histogram chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For each of the 4 features, show its distribution in a matplotlib Histogram chart. \n",
    "# (You should get 4 different histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the previous histogram visualization, but show color according to each Species\n",
    "# (Use the same colors as before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the histograms you just plotted, what could probably be the best feature(s) to differenciate the 3 different Iris Species? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your observations.\n",
    "\n",
    "\"\"\"\n",
    "You can use a multi-line comment to write big text in Python, and answer the question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we use Pandas to generate our histograms. It can be useful to have a visualisation of the distribution of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplots\n",
    "\n",
    "With seaborn, you can use `pairplot` with `kind=\"reg\"` to show linear relationship between different variables.\n",
    "\n",
    "Below, we use the same feature as Y axis, and use the 3 left features in X axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, \n",
    "             x_vars=[\"PetalLength (cm)\", \"PetalWidth (cm)\", \"SepalWidth (cm)\"], \n",
    "             y_vars=[\"SepalLength (cm)\"],\n",
    "             hue=\"Species\",\n",
    "             height=5, aspect=.8, kind=\"reg\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select all your dataset, let Seaborn determine what to show and see what happens. (Not recommended if you have a lot of features, i.e. the Pokemon Go dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"Species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "\n",
    "The correlation of each feature to another plays an important role. \n",
    "\n",
    "If there are features and many of the features are highly correlated, then training an algorithm with all the featues will reduce the accuracy of the resulting model.\n",
    "\n",
    "You can get the correlation matrix of all numerical features of a pandas.DataFrame using `.corr()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap with Matplotlib\n",
    "\n",
    "Let's visualize our correlation matrix with an Heatmap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[:-1]\n",
    "values = df.corr().values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "im = ax.imshow(values, \n",
    "               cmap=\"cubehelix_r\")\n",
    "\n",
    "# Show X and Y ticks & label them\n",
    "ax.set_xticks(np.arange(len(features)))\n",
    "ax.set_yticks(np.arange(len(features)))\n",
    "\n",
    "ax.set_xticklabels(features)\n",
    "ax.set_yticklabels(features)\n",
    "\n",
    "# Create text annotations.\n",
    "for i in range(len(features)):\n",
    "    for j in range(len(features)):\n",
    "        color = \"w\" if values[i, j] > 0.5 else \"black\"\n",
    "        text = ax.text(j, i, round(values[i, j], 2), \n",
    "                       ha=\"center\", \n",
    "                       va=\"center\", \n",
    "                       color=color)\n",
    "\n",
    "ax.set_title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap with Seaborn\n",
    "\n",
    "Spoiler: it's much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "sns.heatmap(df.corr(), \n",
    "            ax=ax, \n",
    "            annot=True, \n",
    "            cmap='cubehelix_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Low correlation value == least correlation. Observations:\n",
    "\n",
    "\n",
    "**Not correlated**:\n",
    "* SepalWidth and PetalWidth\n",
    "* SepalWidth and PetalLength\n",
    "\n",
    "**Highly correlated**:\n",
    "* PetalWidth and PetalLength\n",
    "* SepalLength and PetalWidth & PetalLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Principal Component Analysis\n",
    "\n",
    "* Normalize\n",
    "* Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features using StandardScaler.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "matrix = scaler.fit_transform(df[features])\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use PCA with `n_components=2` to transform your 4D dataset `matrix` into a 2D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use your 2D dataset to plot a 2D scatter visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use PCA with `n_components=3` to transform your 4D dataset `matrix` into a 3D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use your 3D dataset to plot a 3D scatter visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)\n",
    "\n",
    "We learnt about Principal Component Analysis (PCA) just before.\n",
    "\n",
    "Both LDA and CPA are **linear transformation** techniques used for **dimensionality reduction**. \n",
    "\n",
    "But PCA is said to be an **unsupervised** algorithm, since it does not need a **target** (class value) to work, contrary to LDA. PCA will compute vectors (axes) that maximize the *variance*, whereas LDA will maximize the *distance* between each class center (the best way to separate classes).\n",
    "\n",
    "LDA follows steps similar from PCA, but differs slightly:\n",
    "\n",
    "* Get all the `d`-dimensions (the features we want to reduce), target excluded\n",
    "1. Get the *mean vectors*, the mean of each feature **for each class**\n",
    "1. Compute **multiple scatter matrices: in-between-class and within-class.**\n",
    "1. Compute their eigenvectors $(e_{1}, e_{2},...,e_{d})$ and corresponding eigenvalues $(λ_{1},λ_{2},...,λ_{d})$ \n",
    "> The eigenvalues tell us the *length* (or *magnitude*) of the eigenvectors, and is a value >= 0.  \n",
    "> If all the eigenvalues are similar in length, it means our dataset is well represented by those features.  \n",
    "> If some eigenvalues are really high, and some are close to zero, it means the latters are less informative. We might consider dropping those features and keep only the higher values to construct the our `k`-dimensional subspace. \n",
    "1. Visualize the eigenvectors on a 3D Plan\n",
    "1. Sort the eigenvectors by decreasing eigenvalues and choose `k` eigenvectors with the largest eigenvalues to form a `d`×`k` dimensional matrix $W$\n",
    "1. Project our `d`-dimensional dataset in the new `k`-dimensional subspace, created from the eigenvectors with the highest eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform String class values to Numerical\n",
    "\n",
    "Remember I told you we could transform String or Boolean value in numerical values?\n",
    "\n",
    "The simpler solution is to do it by hand by creating a label dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = dict()\n",
    "\n",
    "for i, species in enumerate(df['Species'].unique()):\n",
    "    label_dict.update({species : i})\n",
    "    \n",
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use `apply` to encode each String class value to a numerical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2num(species: str):\n",
    "    return label_dict[species]\n",
    "\n",
    "df['Species'] = df['Species'].apply(str2num)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[features].values  # features\n",
    "y = df['Species'].values  # target\n",
    "y, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mean vectors for each feature and each class\n",
    "\n",
    "Since we have 4 features and 3 class, our `mean_vectors` matrix will be of shape (3, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vectors = []\n",
    "\n",
    "# TODO: Iterate over the 3 classes and append the 4 features means to mean_vectors. You should get a shape of (3, 4).\n",
    "\n",
    "mean_vectors = np.array(mean_vectors)\n",
    "print(mean_vectors.shape)\n",
    "print(mean_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Scatter Matrices\n",
    "\n",
    "We have 4 features, so we will compute two (4, 4)-dimensional matrices: the **in-between-class** and the **within-class** scatter matrices.\n",
    "\n",
    "## Within-Class Scatter Mattrix\n",
    "\n",
    "The within-class scatter matrix $ S_{W} $ is computed as:\n",
    "\n",
    "$$ S_{W} = {\\displaystyle{\\sum_{i=1}^{c} S_{i}}} $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$ S_{i} = {\\displaystyle{\\sum_{i=1}^{n} ({\\displaystyle{\\vec {z_{i}} \\cdot \\vec {z_{i}} }}) }} $$\n",
    "\n",
    "Where $ \\vec{z_{i}} $ is the vector containing the normalized values $ x_{i} - {\\bar {x_{i}}} $ for the feature $i$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = len(features)\n",
    "\n",
    "# Within-Class\n",
    "wc_scatter_matrix = np.zeros((d,d)) \n",
    "wc_scatter_matrix\n",
    "\n",
    "# For each class\n",
    "for c, mean in zip(label_dict.values(), mean_vectors):\n",
    "    column_mean = mean.reshape(4, 1)\n",
    "    class_sc_matrix = np.zeros((d, d))\n",
    "    # For each sample\n",
    "    for sample in df[df['Species'] == c][features].values:\n",
    "        column_vec = sample.reshape(4, 1)\n",
    "        normalized_vec = column_vec - column_mean\n",
    "        class_sc_matrix += normalized_vec.dot(normalized_vec.T)\n",
    "        \n",
    "    wc_scatter_matrix += class_sc_matrix\n",
    "\n",
    "wc_scatter_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Between Class Scatter Mattrix\n",
    "\n",
    "The within-class scatter matrix $ S_{B} $ is the sum of the dot product of each normalized mean, with each dot product multiplied by the size.\n",
    "\n",
    "$$ S_{B} = {\\displaystyle{\\sum_{i=1}^{c} N_{i} ({\\displaystyle{\\vec {m} \\cdot \\vec {m} }})  }} $$\n",
    "\n",
    "Where \n",
    "* $N_{i}$ is the \n",
    "* $ \\vec{m} $ is the vector of the normalized means for the class i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Between-Class\n",
    "bc_scatter_matrix = np.zeros((d,d)) \n",
    "\n",
    "overall_mean = np.mean(df[features]).values.reshape((d, 1))\n",
    "\n",
    "# For mean\n",
    "for i, mean in enumerate(mean_vectors):\n",
    "    column_mean = mean.reshape(4, 1)\n",
    "    ni = df[df['Species'] == c].shape[0]\n",
    "    normalized_vec = column_mean - overall_mean\n",
    "    bc_scatter_matrix += ni * normalized_vec.dot(normalized_vec.T)\n",
    "\n",
    "bc_scatter_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute eigenvalues and eigenvectors\n",
    "\n",
    "Task: Use `np.linalg.eig` to get the eigenvalue and eigenvectors of $ S_{W} \\cdot S_{B} $ (the dot product of the Within-Class scatter matrix and the Between-Class catter matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the eigenvalues and vectors of the dot product of the matrices\n",
    "\n",
    "eig_vals, eig_vecs = 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the k best features\n",
    "\n",
    "Exactly as we did for the PCA, select the `k = 2` best features based on eigen values.\n",
    "\n",
    "Use `np.hstack` to create the matrix $ W $, `matrix_w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "\n",
    "# TODO: Create the matrix_w.\n",
    "matrix_w = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project to the new subspace\n",
    "\n",
    "Transform the samples onto the new subspace via the equation $y = ^{t}Wx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the x_lda value using `x` and `matrix_w`.\n",
    "x_lda = np.array()\n",
    "x_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lda(x_lda):\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "    for label, color in zip(label_dict.keys(), ('blue', 'red', 'green')):\n",
    "        plt.scatter(x=x_lda[df['Species'] == label_dict[label]][:, 0],\n",
    "                    y=x_lda[df['Species'] == label_dict[label]][:, 1],\n",
    "                    marker=\"*\",\n",
    "                    color=color,\n",
    "                    alpha=0.5,\n",
    "                    label=label)\n",
    "\n",
    "    plt.title('LDA Projection')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_lda(x_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with LDA from Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "x_lda_sklearn = lda.fit_transform(x, y)\n",
    "\n",
    "plot_lda(x_lda_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks: Plot your LDA-transformed data using Seaborn into:\n",
    "* A scatterplot\n",
    "* A linear regression plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert `x_lda` to a DataFrame and print the Seaborn scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the same DataFrame to print it using `lmplot`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
