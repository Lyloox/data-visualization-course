{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Data Analysis\n",
    "\n",
    "Python is an efficient tool for implementing numerous statistical data analysis operations.\n",
    "\n",
    "NumPy is really useful in manipulating data and providing mathematical and statistic tools -- we will discover some more today.\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "In data science, we do not use the terms `rows` and `columns` to refer to a dataset. \n",
    "* The rows are called **samples**. For example, in our PokémonGo dataset, each sample was a Pokémon appearance.\n",
    "* The columns are called **features**. Each feature describe a particular aspect of the sample. In our Pokémon Go dataset, it was the appaered Time, Hour, or Minute. It was a boolean of if it was closeToWater or not. It was the current weather in the game. In other datasets (with different samples), it could be the length of a sentence, the frequency of a word, the number of legs of an animal, a creation date, a GPS location, ...\n",
    "* The **dimension** of a dataset is its number of features. If a dataset have 3 dimensions, we can call it a *3-dimensional* dataset.\n",
    "\n",
    "\n",
    "# Principal component analysis (PCA)\n",
    "\n",
    "The main purpose of PCA is **dimensionality reduction**.\n",
    "\n",
    "If we have a `d`-dimensional dataset, we can try to:\n",
    "* Reduce it to 2 or 3 dimensional dataset, so we can visualize it, and hopefully understand the data better\n",
    "* Reduce it to a `k`-dimensional dataset, with `k` < `d`, so the computational cost of machine learning algorithms is reduced (i.e. for classification).\n",
    "\n",
    "The `k`-dimensional *subspace* is a **projection** of our `d`-dimensional dataset.\n",
    "\n",
    "To reduces the dimension, one needs to understand the relation between each features. This can done by computing a **scatter matrix** or a **covariance matrix**. We will use them to select which features to keep in our projection.\n",
    "\n",
    "Here, we will implement PCA from scratch so we better understand how this algorithm works. We will go through these steps (they will be explained in details on the go):\n",
    "\n",
    "* Get all the `d`-dimensions (the features we want to reduce)\n",
    "1. Get the *mean vector*, the mean of each feature\n",
    "1. Compute the covariance matrix or the scatter matrix of the dataset\n",
    "1. Compute their eigenvectors $(e_{1}, e_{2},...,e_{d})$ and corresponding eigenvalues $(λ_{1},λ_{2},...,λ_{d})$ (in French, the *vecteurs propres* and *valeurs propres*)  \n",
    "> The eigenvalues tell us the *length* (or *magnitude*) of the eigenvectors, and is a value >= 0.  \n",
    "> If all the eigenvalues are similar in length, it means our dataset is well represented by those features.  \n",
    "> If some eigenvalues are really high, and some are close to zero, it means the latters are less informative. We might consider dropping those features and keep only the higher values to construct the our `k`-dimensional subspace. \n",
    "1. Visualize the eigenvectors on a 3D Plan\n",
    "1. Sort the eigenvectors by decreasing eigenvalues and choose `k` eigenvectors with the largest eigenvalues to form a `d`×`k` dimensional matrix $W$\n",
    "1. Project our `d`-dimensional dataset in the new `k`-dimensioanl subspace, created from the eigenvectors with the highest eigenvalues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "import random  #  <-- a new library has appeared!\n",
    "from pprint import pprint  #  <-- this is a built-in function to \"pretty print\", useful to visualize datastructures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "\n",
    "First, we will create a small dataset of `nb_cats` cat samples, with a few features for each cat.\n",
    "\n",
    "To generate our dataset, we will use the function `random.choice(L)` that randomly choose an option in the list `L`. So we will list the options in lists in `possible_values`.\n",
    "\n",
    "The built-in Python function `range()` gives a `generator` of values (kind of equivalent to a `list`, but with *lazy* generation of elements). \n",
    "* If you give it one argument, `range(stop)` will give you each value from 0 to `stop-1`, iterating by 1. \n",
    "* If you give it two arguments, `range(start, stop)` will give you values from `start` to `stop-1`, iterating by 1. \n",
    "* If you give it three arguments, `range(start, stop, step)` will give you values from `start` to `stop-1`, iterating by `step`.\n",
    "\n",
    "The function `arange()` from NumPy works the same, but authorize the use of type `float` instead of `int` as arguments.\n",
    "\n",
    "Source of the Cat sizes: https://www.cat-toure.com/pages/size-chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_values = {\n",
    "    \"EyeColor\": [\"cyan\", \"lime\", \"black\", \"sienna\", \"gold\"],\n",
    "    \"CoatColor\": [\"orange\", \"black\"],\n",
    "    \"ChestLength (cm)\": range(28, 52),\n",
    "    \"Length (cm)\": range(26, 42),\n",
    "    \"Weight (kg)\": np.arange(2, 9, 0.5)\n",
    "}\n",
    "\n",
    "nb_cats = 10\n",
    "\n",
    "cats = []\n",
    "for i in range(nb_cats):\n",
    "    cat_sample = []\n",
    "    for col, values in possible_values.items():\n",
    "        cat_sample.append(random.choice(values))\n",
    "    print(f\"Sample {i}:\", cat_sample)\n",
    "    cats.append(cat_sample)\n",
    "\n",
    "print(\"\\nResulting cats dataset:\")\n",
    "pprint(cats)  #  <-- look how we used pretty printing here. \n",
    "# Try to use a basic `print` to see how ugly it is otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating functions\n",
    "\n",
    "Instead of modifying the value `nb_cats` each time we want to generate a dataset with less or more cats, we can create a function to call later on.\n",
    "\n",
    "\n",
    "### Good practice: Typing \n",
    "\n",
    "Type your functions so Python can warn you when the types you give in argument are not those wanted by your function.\n",
    "\n",
    "Basic data types in Python are:\n",
    "* Integers: `int`\n",
    "* Floating-Point numbers: `float`\n",
    "* Strings: `str`\n",
    "* Boolean: `bool`\n",
    "\n",
    "More types can be found in the built-in Python modules `typing`.\n",
    "\n",
    "Basic data structures:\n",
    "* Lists: `list` or `typing.List`. With the latter, you can define the elements inside the list. For example, `List[str]` is a list of String elements.\n",
    "* Dictionaries: `dict` or `typing.Dict`. With the latter, you can define the types of the Keys and Values. For example, `Dict[str, int]` is a dictionary with Strings keys containing integer values. \n",
    "\n",
    "You can combine data types definitions. For example, `Dict[str, List[str]]` is a dictionary that takes Strings as keys, and a list of Strings elements as values.\n",
    "\n",
    "Let's rearrange our code above as a typed function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)  # This seed allow us to have consistency.\n",
    "\n",
    "def create_cats_dataset(nb_cats: int) -> pd.DataFrame:  # Here, we return a pd.DataFrame.\n",
    "    cats = []\n",
    "    for i in range(nb_cats):\n",
    "        cat_sample = []\n",
    "        for values in possible_values.values():\n",
    "            cat_sample.append(random.choice(values))\n",
    "        cats.append(cat_sample)\n",
    "    return pd.DataFrame(cats, columns=possible_values.keys())\n",
    "\n",
    "print(\"\\nCat dataset 1:\")\n",
    "tiny_dataset = create_cats_dataset(5)\n",
    "pprint(tiny_dataset)\n",
    "\n",
    "print(\"\\nCat dataset 2:\")\n",
    "small_dataset = create_cats_dataset(25)\n",
    "pprint(small_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Normalisation\n",
    "\n",
    "Before applying PCA, we need to **normalize** our data.\n",
    "\n",
    "The simpliest definition of normalisation is scaling values compared to the average. For each value $x$\n",
    "\n",
    "$${\\displaystyle z= x-{\\bar {x}}} $$\n",
    "\n",
    "where $\\bar {x}$ is the mean of the sample.\n",
    "\n",
    "This way, values that are equal to the average will be equal to 0. \n",
    "\n",
    "First, let's select normalizable features - they are the ones with numeric values. \n",
    "\n",
    "We could also transform String or Boolean values in numerical values, but we will see that later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3 # We take the 3 last features, because they are numerical. This is our initial `d`-dimensional dataset.\n",
    "\n",
    "normalizable_columns = list(possible_values.keys())[-d:]  # Look how we used the Slicing syntax here!\n",
    "print(normalizable_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Visualization using Matplotlib\n",
    "\n",
    "We have 3 numerical features here - let's visualize them in 3D!\n",
    "\n",
    "It's easy, you just have to:\n",
    "\n",
    "* import mplot3d `from mpl_toolkits import mplot3d`\n",
    "* specify to `fig.add_subplot` that you want a `projection='3d'`\n",
    "* gives 3 arguments to your plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(small_dataset[normalizable_columns[0]],\n",
    "           small_dataset[normalizable_columns[1]],\n",
    "           small_dataset[normalizable_columns[2]], \n",
    "           c=small_dataset[\"CoatColor\"])\n",
    "\n",
    "ax.set(xlabel=normalizable_columns[0],\n",
    "       ylabel=normalizable_columns[1],\n",
    "       zlabel=normalizable_columns[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: c=small_dataset[\"CoatColor\"] works because colors \"gold\" and \"black\" are [recognized Matplotlib colors](https://matplotlib.org/3.1.0/gallery/color/named_colors.html).\n",
    "\n",
    "Each color point is the fur color of the cat represented in this 3D view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of our `small_dataset`, the means are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vector = small_dataset[normalizable_columns].mean()\n",
    "mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset[normalizable_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `apply` function from Pandas (you should be familiar with it now!) to substract each value from its average.\n",
    "\n",
    "To keep our initial data from our transformed data, we will copy our data in a new DataFrame, `small_dataset_transformed`.\n",
    "\n",
    "### Hint\n",
    "Instead of defining a function only for this purpose, we can use a **lambda function**.\n",
    "\n",
    "It's also called a *anonymous function*: it's a function without a name, that we define using\n",
    "\n",
    "> lambda *arguments* : *expression*\n",
    "\n",
    "\n",
    "For exemple\n",
    "\n",
    "`lambda x: x + 5`\n",
    "\n",
    "is equivalent to\n",
    "\n",
    "`def plus_five(x):\n",
    "    return x + 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset_transformed = small_dataset.copy()\n",
    "\n",
    "# TODO: Use the `apply` function from Pandas to substract each value from its average.\n",
    "for col in normalizable_columns:  \n",
    "    small_dataset_transformed[col] = small_dataset[col]\n",
    "\n",
    "small_dataset_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Compute the matrices\n",
    "\n",
    "\n",
    "## Method 1: The Covariance Matrix\n",
    "\n",
    "### Using `cov` from NumPy\n",
    "\n",
    "From the documentation of the `cov` method\n",
    "\n",
    "> Signature:\n",
    "np.cov(\n",
    "    m,\n",
    "    y=None,\n",
    "    rowvar=True,\n",
    "    bias=False,\n",
    "    ddof=None,\n",
    "    fweights=None,\n",
    "    aweights=None,\n",
    ")\n",
    "> Parameters  \n",
    "> ----------  \n",
    "> m : array_like  \n",
    ">    A 1-D or 2-D array containing multiple variables and observations.  \n",
    ">    Each row of `m` represents a variable, and each column a single  \n",
    ">    observation of all those variables. Also see `rowvar` below.  \n",
    "  \n",
    "  \n",
    "So, let's transform our Pandas DataFrame into an array-like with each row being a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset_matrix = small_dataset_transformed[normalizable_columns].values\n",
    "\n",
    "# Pandas and NumPy can be used together: using `.values` on a DataFrame gives us an NumPy array.\n",
    "print(type(small_dataset_matrix))\n",
    "print(small_dataset_matrix.shape)\n",
    "\n",
    "small_dataset_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose\n",
    "\n",
    "But the shape is incorrect: we have a shape (n_samples, n_features) whereas `np.cov` waits for a (n_features, n_samples) shape.\n",
    "\n",
    "if `Array` is your NumPy array, you can use the function `np.transpose(Array)` or `Array.T` to transpose it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.transpose(small_dataset_matrix))\n",
    "print(small_dataset_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = np.cov(small_dataset_matrix.T)\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the second column of the `cov_matrix`.\n",
    "\n",
    "print(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Now, transpose the cov_matrix.\n",
    "\n",
    "print(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's normal if it gives you the same result. \n",
    "\n",
    "Every covariance matrix is **symetric**, so if $M$ is a covariance matrix, $^{t}M = M$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Scatter Matrix\n",
    "\n",
    "You can choose between the two methods. \n",
    "\n",
    "The scatter matrix is defined as an estimation of covariance matrix. Usually, people use the Scatter Matrix only when the Covariance matrix can't be computed (or is too costly to).\n",
    "\n",
    "Our Scatter Matrix, like our Covariance Matrix, will be of shape $(d, d)$ if there is $d$ variables.\n",
    "\n",
    "The scatter matrix is the sum of the dot product of each sample.\n",
    "\n",
    "### Reminder: Dot Product\n",
    "\n",
    "The **dot product** is the *produit scalaire* in French.\n",
    "\n",
    "If the vectors $\\vec x$ and $\\vec y$ have coordinates \n",
    "$(x_{1}, x_{2}, x_{3})$ and \n",
    "$(y_{1}, y_{2}, y_{3})$, their dot product is:\n",
    " \n",
    "$${\\displaystyle {\\vec {x}}\\cdot {\\vec {y}}=x_{1}y_{1}+x_{2}y_{2}+x_{3}y_{3}}.$$\n",
    "\n",
    "In matrix writing:\n",
    "\n",
    "$$ {\\displaystyle {\\vec {x}}\\cdot {\\vec {y}} = \n",
    "{}^{t}X~Y= \n",
    "{\\begin{pmatrix}x_{1}&x_{2}&x_{3}\\end{pmatrix}}{\\begin{pmatrix}y_{1}\\\\y_{2}\\\\y_{3}\\end{pmatrix}}=\n",
    "x_{1}y_{1}+x_{2}y_{2}+x_{3}y_{3}} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix = np.zeros((d,d)) # Create a NumPy array of shape (d, d) filled of 0.\n",
    "\n",
    "for i in range(small_dataset_matrix.shape[0]):\n",
    "    scatter_matrix += (small_dataset_matrix[i].reshape(d,1)).dot((small_dataset_matrix[i].reshape(d,1)).T)\n",
    "\n",
    "scatter_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Compute the eigenvectors and eigenvalues\n",
    "\n",
    "\n",
    "Again, NumPy come for the rescue by providing us the methods `np.linalg.eig` to compute the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvectors and eigenvalues for the from the scatter matrix\n",
    "eig_val_sc, eig_vec_sc = np.linalg.eig(scatter_matrix)\n",
    "eig_val_sc, eig_vec_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvectors and eigenvalues for the from the covariance matrix\n",
    "eig_val_cov, eig_vec_cov = np.linalg.eig(cov_matrix)\n",
    "eig_val_cov, eig_vec_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking\n",
    "\n",
    "If the eigenvalues and vectors computation is correct, it should statisfy the equation\n",
    "\n",
    "$$Σv=λv$$\n",
    "\n",
    "where  \n",
    "$Σ$ = Covariance matrix  \n",
    "$v$ = Eigenvector  \n",
    "$λ$ = Eigenvalue  \n",
    "\n",
    "To test the equality, we will use the method `np.testing.assert_array_almost_equal(x, y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eig_val_sc)):\n",
    "    eigv = eig_vec_sc[:,i].reshape(1,3).T\n",
    "    np.testing.assert_array_almost_equal(scatter_matrix.dot(eigv), \n",
    "                                         eig_val_sc[i] * eigv,\n",
    "                                         decimal=2, \n",
    "                                         verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the `d` eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eig_val_sc)):\n",
    "    eigvec_sc = eig_vec_sc[:,i]\n",
    "    eigvec_cov = eig_vec_cov[:,i]\n",
    "    \n",
    "    print(f\"\\nEigenvector {i}\")\n",
    "    print(f\"  Eigenvector from scatter matrix: {eigvec_sc}\")\n",
    "    print(f\"  Eigenvector from covariance matrix: {eigvec_cov}\")\n",
    "    print(f\"  Eigenvalue from scatter matrix: {eig_val_sc[i]}\")\n",
    "    print(f\"  Eigenvalue from covariance matrix: {eig_val_cov[i]}\")\n",
    "    print(f\"  Scaling factor: {round(eig_val_sc[i]/eig_val_cov[i], 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot our points, like before\n",
    "ax.scatter(small_dataset_transformed[normalizable_columns[0]],\n",
    "           small_dataset_transformed[normalizable_columns[1]],\n",
    "           small_dataset_transformed[normalizable_columns[2]], \n",
    "           c=small_dataset[\"CoatColor\"])\n",
    "\n",
    "origin = [0, 0, 0] # Data were normalized, so everything is centered around 0\n",
    "\n",
    "# Use `ax.quiver` to plot each vector (array)\n",
    "for v, color in zip(eig_vec_sc.T, ['r', 'g', 'b']):\n",
    "    ax.quiver(*origin, v[0], v[1], v[2], color=color)\n",
    "\n",
    "ax.set(xlabel=normalizable_columns[0],\n",
    "       ylabel=normalizable_columns[1],\n",
    "       zlabel=normalizable_columns[2])\n",
    "\n",
    "plt.title('Eigenvectors')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Choose the `k` higher eigenvalues\n",
    "\n",
    "We just printed the vectors for each of our 3 features.\n",
    "\n",
    "In order to project these 3 features onto a smaller subspace, which will be `k` = 2, we should keep the `k` eigenvectors with the highest eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tuple of corresponding (eigenvalue, eigenvector)\n",
    "eig_pairs = [(val, vec) for val, vec in zip(eig_val_cov, eig_vec_cov)]\n",
    "eig_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(eig_pairs, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "k_eig_vectors = sorted(eig_pairs, reverse=True)[:k]  # Slicing syntax again\n",
    "k_eig_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use `np.hstack` to stack horizontally each selected eigenvectors to our new matrix $W$ `matrix_w`.\n",
    "\n",
    "This matrix will be used to make our projection from `d`-dimensional datasets to a `k`-dimensional one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_w = np.hstack([k_eig_vectors[0][1].reshape(d, 1), \n",
    "                      k_eig_vectors[1][1].reshape(d, 1)])\n",
    "matrix_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final step: Project onto the new subspace\n",
    "\n",
    "\n",
    "For that, we need to create a (`d`,`k`) shaped matrix $W$.\n",
    "\n",
    "We then transform our samples onto the new subspace via the equation $y = ^{t}Wx$\n",
    "\n",
    "* $x$ being the samples `small_dataset`, of shape (`d`, n_samples).\n",
    "* $y$ being our transformed samples of shape (`k`, n_samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(small_dataset_matrix.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Project `small_dataset_matrix` in `k_subspace` by using a dot product on matrix_w.\n",
    "\n",
    "## Hint: You can ctrl+f \"dot\" in this document to see previous usage.\n",
    "\n",
    "k_subspace = small_dataset_matrix.T\n",
    "\n",
    "print(k_subspace.shape)  # Should be (2, 25)\n",
    "k_subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your resulting shape should be `(k, n_samples)`.\n",
    "\n",
    "\n",
    "It's `(2, 25)` if you didn't modify the variable `k` and the number of cats generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data in the new subspace\n",
    "\n",
    "We're finished! \n",
    "\n",
    "We just used the two vectors as axis for our space, and `k_subspace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(18, 12))\n",
    "\n",
    "plt.scatter(k_subspace[0], k_subspace[1], c=small_dataset[\"CoatColor\"])\n",
    "\n",
    "plt.title('small_dataset on the new subspace')\n",
    "\n",
    "ax.set(xlabel=\"New X axis\",\n",
    "       ylabel=\"New Y axis\")\n",
    "       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wonderful!\n",
    "\n",
    "Thank you for following this tutorial and doing your Principal Component Analysis almost by yourself.\n",
    "\n",
    "I hope you understood how it works -- it's important to understand what algorithms are doing when we use them.\n",
    "\n",
    "Now that you are familiar with it, I have good news for you.\n",
    "\n",
    "The PCA is implemented in the `scikit-learn` library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(small_dataset_matrix)\n",
    "\n",
    "principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(18, 12))\n",
    "\n",
    "plt.scatter(principal_components[:, 0], principal_components[:, 1], c=small_dataset[\"CoatColor\"])\n",
    "\n",
    "plt.title('small_dataset on the new subspace using sklearn')\n",
    "\n",
    "ax.set(xlabel=\"New X axis\",\n",
    "       ylabel=\"New Y axis\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
